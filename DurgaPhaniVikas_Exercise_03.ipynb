{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DurgaPhaniVikas/DurgaPhaniVikas_INFO5731_-Fall2023/blob/main/DurgaPhaniVikas_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4d482fa7-84be-4511-bca7-1d53c8b1d4ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nNow let us dicuss an interesting text classification task that is sentiment analysis of customer reviews for a restaurant. Now\\nwe will be classifing the customer reviews as either positive, negative, or neutral based on the sentiment that was mentioned in the reviews as text.\\nI am going to use these five features listed below that i believe that they are useful to build a machine learning model\\n\\n\\nBag of Words (BoW)\\nUsing this method, each review is represented as a vector of word frequencies (unigrams or n-grams) across the entire dataset.\\nReason: Because BoW features record the word frequency in reviews, the model can learn whether words or phrases are connected to good or\\nnegative modes.\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)\\nSimilar to BoW, TF-IDF measures word frequencies but gives terms that are significant in a particular review but uncommon across all reviews\\nhigher weights.\\nReason:Because TF-IDF features can help identify terms that are discriminative for a particular review, lessening the influence of common words.\\n\\nSentiment Lexicon:\\nA lexicon-based strategy using sentiment lexicons (lists of words with corresponding sentiment ratings, such as positive and negative terms)\\nhas the following features.\\nReason: By using sentiment lexicons, the model can directly take into account words that convey emotion, making it simpler to determine the\\nsentiment that was intended to be conveyed in the text.\\n\\nPart-of-Speech (POS):\\nPOS tagging to identify the nouns, adjectives, and verbs that make up the text's grammatical structure.\\nReason: Because POS traits may be helpful in identifying grammatical patterns in reviews. For instance, a review that uses a lot of positive \\nadjectives could be considered good.\\n\\nSentence Length and Structure:\\nThe review's structure can be described using metrics like the average sentence length, the number of paragraphs, or the use of punctuation.\\nReason: The length and organization of the sentences can give hints about how expressive the review is and perhaps even convey the sentiment. For instance, \\nsuccinct, direct phrases may be more suggestive of negative feeling.\\n\\n\\n\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Now let us dicuss an interesting text classification task that is sentiment analysis of customer reviews for a restaurant. Now\n",
        "we will be classifing the customer reviews for a restaurent as either positive, negative, or neutral based on the sentiment that was mentioned in the reviews as text.\n",
        "I am going to use these five features listed below that i believe that they are useful to build a machine learning model\n",
        "\n",
        "\n",
        "Bag of Words (BoW)\n",
        "Using this method, each review is represented as a vector of word frequencies (unigrams or n-grams) across the entire dataset.\n",
        "Reason: Because BoW features record the word frequency in reviews, the model can learn whether words or phrases are connected to good or\n",
        "negative modes.\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "Similar to BoW, TF-IDF measures word frequencies but gives terms that are significant in a particular review but uncommon across all reviews\n",
        "higher weights.\n",
        "Reason:Because TF-IDF features can help identify terms that are discriminative for a particular review, lessening the influence of common words.\n",
        "\n",
        "Sentiment Lexicon:\n",
        "A lexicon-based strategy using sentiment lexicons (lists of words with corresponding sentiment ratings, such as positive and negative terms)\n",
        "has the following features.\n",
        "Reason: By using sentiment lexicons, the model can directly take into account words that convey emotion, making it simpler to determine the\n",
        "sentiment that was intended to be conveyed in the text.\n",
        "\n",
        "Part-of-Speech (POS):\n",
        "POS tagging to identify the nouns, adjectives, and verbs that make up the text's grammatical structure.\n",
        "Reason: Because POS traits may be helpful in identifying grammatical patterns in reviews. For instance, a review that uses a lot of positive\n",
        "adjectives could be considered good.\n",
        "\n",
        "Sentence Length and Structure:\n",
        "The review's structure can be described using metrics like the average sentence length, the number of paragraphs, or the use of punctuation.\n",
        "Reason: The length and organization of the sentences can give hints about how expressive the review is and perhaps even convey the sentiment. For instance,\n",
        "succinct, direct phrases may be more suggestive of negative feeling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hki_5JTvUwRu",
        "outputId": "09030cfd-4572-4713-acbe-95cec9aa49f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWbwT2dpXCk5",
        "outputId": "d01e2ad6-1e7a-414b-f175-df95be976a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkmzwO_jXLIo",
        "outputId": "e57fdc87-0d12-4d84-bb10-fc502e5f3927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TZFeJALtUaD",
        "outputId": "473b0740-1cd0-43e1-a0ae-0e1c08b7dd85"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cba1f1d-68c1-44e4-e83e-ba6d95fcdac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textblob\n",
        "import spacy\n",
        "import pandas as pd\n",
        "# sample reviews data of the restaurent\n",
        "sample_review_data = [\n",
        "    \"The food at this restaurant is amazing! I love the biriyani.\",\n",
        "    \"The restaurent ambience is pleasant but need to work on the flavours.\",\n",
        "    \"Service was terrible. Waited for an hour and the waiter was rude.\",\n",
        "    \"I don't have much to say about this place. It was average. But biriyani was good\",\n",
        "    \"The ambiance here is cozy and inviting. I enjoyed my meal.\",\n",
        "]\n",
        "\n",
        "# Here we are initializing spaCy and NLTK\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "sentmental_intensity_analyzer = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words (BoW)\n",
        "count_vect = CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
        "bow_feat = count_vect.fit_transform(sample_review_data)\n",
        "bow_df = pd.DataFrame(bow_feat.toarray(), columns=count_vect.get_feature_names_out())\n",
        "print(\"Bag of Words (BoW):\\n\", bow_df)\n",
        "\n",
        "#Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "tf_idf_vectorizer = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
        "tf_idf_features = tf_idf_vectorizer.fit_transform(sample_review_data )\n",
        "tf_idf_df = pd.DataFrame(tf_idf_features.toarray(), columns=tf_idf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF:\\n\", tf_idf_df)\n",
        "\n",
        "#Sentiment Lexicons (VADER sentiment scores)\n",
        "\n",
        "review_sent_scores = [sentmental_intensity_analyzer.polarity_scores(text) for text in sample_review_data]\n",
        "review_sent_df = pd.DataFrame(review_sent_scores)\n",
        "print(\"\\nSentiment Lexicons (VADER):\\n\", review_sent_df)\n",
        "\n",
        "#Part-of-Speech (POS) Tags\n",
        "Part_of_Speech_tags = [nltk.pos_tag(nltk.word_tokenize(text)) for text in sample_review_data]\n",
        "print(\"\\nPart-of-Speech (POS) Tags:\\n\", Part_of_Speech_tags)\n",
        "\n",
        "#Sentence Length and Punctuation Statistics\n",
        "length_of_sentences = [len(text.split()) for text in sample_review_data]\n",
        "count_of_punc = [text.count(\"!\") for text in sample_review_data]\n",
        "mode_of_review_df = pd.DataFrame({\"Sentence Length\": length_of_sentences , \"Exclamation Count\": count_of_punc})\n",
        "print(\"\\nSentence Length and Punctuation Statistics:\\n\", mode_of_review_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q5owN9atbcX",
        "outputId": "42f58dc0-6223-4cda-d51c-d6c53cf0674b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW):\n",
            "    amazing  ambiance  ambience  average  biriyani  cozy  enjoyed  flavours  \\\n",
            "0        1         0         0        0         1     0        0         0   \n",
            "1        0         0         1        0         0     0        0         1   \n",
            "2        0         0         0        0         0     0        0         0   \n",
            "3        0         0         0        1         1     0        0         0   \n",
            "4        0         1         0        0         0     1        1         0   \n",
            "\n",
            "   food  good  ...  pleasant  restaurant  restaurent  rude  say  service  \\\n",
            "0     1     0  ...         0           1           0     0    0        0   \n",
            "1     0     0  ...         1           0           1     0    0        0   \n",
            "2     0     0  ...         0           0           0     1    0        1   \n",
            "3     0     1  ...         0           0           0     0    1        0   \n",
            "4     0     0  ...         0           0           0     0    0        0   \n",
            "\n",
            "   terrible  waited  waiter  work  \n",
            "0         0       0       0     0  \n",
            "1         0       0       0     1  \n",
            "2         1       1       1     0  \n",
            "3         0       0       0     0  \n",
            "4         0       0       0     0  \n",
            "\n",
            "[5 rows x 27 columns]\n",
            "\n",
            "TF-IDF:\n",
            "     amazing  ambiance  ambience   average  biriyani      cozy   enjoyed  \\\n",
            "0  0.463693  0.000000  0.000000  0.000000  0.374105  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.408248  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.000000  0.420669  0.339393  0.000000  0.000000   \n",
            "4  0.000000  0.447214  0.000000  0.000000  0.000000  0.447214  0.447214   \n",
            "\n",
            "   flavours      food      good  ...  pleasant  restaurant  restaurent  \\\n",
            "0  0.000000  0.463693  0.000000  ...  0.000000    0.463693    0.000000   \n",
            "1  0.408248  0.000000  0.000000  ...  0.408248    0.000000    0.408248   \n",
            "2  0.000000  0.000000  0.000000  ...  0.000000    0.000000    0.000000   \n",
            "3  0.000000  0.000000  0.420669  ...  0.000000    0.000000    0.000000   \n",
            "4  0.000000  0.000000  0.000000  ...  0.000000    0.000000    0.000000   \n",
            "\n",
            "       rude       say   service  terrible    waited    waiter      work  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.408248  \n",
            "2  0.408248  0.000000  0.408248  0.408248  0.408248  0.408248  0.000000  \n",
            "3  0.000000  0.420669  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[5 rows x 27 columns]\n",
            "\n",
            "Sentiment Lexicons (VADER):\n",
            "      neg    neu    pos  compound\n",
            "0  0.000  0.491  0.509    0.8516\n",
            "1  0.000  0.837  0.163    0.2846\n",
            "2  0.379  0.621  0.000   -0.7269\n",
            "3  0.000  0.784  0.216    0.5927\n",
            "4  0.000  0.588  0.412    0.6808\n",
            "\n",
            "Part-of-Speech (POS) Tags:\n",
            " [[('The', 'DT'), ('food', 'NN'), ('at', 'IN'), ('this', 'DT'), ('restaurant', 'NN'), ('is', 'VBZ'), ('amazing', 'JJ'), ('!', '.'), ('I', 'PRP'), ('love', 'VBP'), ('the', 'DT'), ('biriyani', 'NN'), ('.', '.')], [('The', 'DT'), ('restaurent', 'NN'), ('ambience', 'NN'), ('is', 'VBZ'), ('pleasant', 'JJ'), ('but', 'CC'), ('need', 'VBP'), ('to', 'TO'), ('work', 'VB'), ('on', 'IN'), ('the', 'DT'), ('flavours', 'NN'), ('.', '.')], [('Service', 'NNP'), ('was', 'VBD'), ('terrible', 'JJ'), ('.', '.'), ('Waited', 'VBN'), ('for', 'IN'), ('an', 'DT'), ('hour', 'NN'), ('and', 'CC'), ('the', 'DT'), ('waiter', 'NN'), ('was', 'VBD'), ('rude', 'NN'), ('.', '.')], [('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('have', 'VB'), ('much', 'JJ'), ('to', 'TO'), ('say', 'VB'), ('about', 'IN'), ('this', 'DT'), ('place', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('average', 'JJ'), ('.', '.'), ('But', 'CC'), ('biriyani', 'NN'), ('was', 'VBD'), ('good', 'JJ')], [('The', 'DT'), ('ambiance', 'NN'), ('here', 'RB'), ('is', 'VBZ'), ('cozy', 'JJ'), ('and', 'CC'), ('inviting', 'VBG'), ('.', '.'), ('I', 'PRP'), ('enjoyed', 'VBD'), ('my', 'PRP$'), ('meal', 'NN'), ('.', '.')]]\n",
            "\n",
            "Sentence Length and Punctuation Statistics:\n",
            "    Sentence Length  Exclamation Count\n",
            "0               11                  1\n",
            "1               12                  0\n",
            "2               12                  0\n",
            "3               16                  0\n",
            "4               11                  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Sample labels\n",
        "samp_labels = ['biriyani', 'ambience', 'restaurent','cozy','waiter']\n",
        "\n",
        "# Convert labels\n",
        "labd = {label: idx for idx, label in enumerate(samp_labels)}\n",
        "num_labels = [labd[label] for label in samp_labels]\n",
        "\n",
        "# Creating a new TF-IDF vectorizer\n",
        "tf_idf_vectorizer = TfidfVectorizer()\n",
        "tf_idf_features = tf_idf_vectorizer.fit_transform(sample_review_data)\n",
        "\n",
        "# training Random Forest classifier\n",
        "rand_forest_cf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rand_forest_cf.fit(tf_idf_features, num_labels)\n",
        "\n",
        "ft_imp = rand_forest_cf.feature_importances_\n",
        "\n",
        "ft_imp_df = pd.DataFrame({'Feature': tf_idf_vectorizer.get_feature_names_out(), 'Importance': ft_imp})\n",
        "\n",
        "sf = ft_imp_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "top_n = 10\n",
        "for idx, row in sf.head(top_n).iterrows():\n",
        "    print(f\"Feature: {row['Feature']}, Importance: {row['Importance']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgkmodzzHXfH",
        "outputId": "755ab312-490f-42b7-824b-430d7f9fc740"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: the, Importance: 0.0832\n",
            "Feature: is, Importance: 0.0674\n",
            "Feature: food, Importance: 0.0437\n",
            "Feature: an, Importance: 0.0381\n",
            "Feature: restaurent, Importance: 0.0350\n",
            "Feature: to, Importance: 0.0318\n",
            "Feature: ambience, Importance: 0.0317\n",
            "Feature: here, Importance: 0.0309\n",
            "Feature: meal, Importance: 0.0278\n",
            "Feature: waited, Importance: 0.0270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwyjXuGYfpfA",
        "outputId": "4fef1e4c-d7ac-48e1-cd93-654dcadd1692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeee34b6-38c8-4586-d994-f005c37d8ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Similarity to Query in descending order:\n",
            "Rank 1: Similarity = 0.7963\n",
            "Text: The food at this restaurant is amazing! I love the biriyani.\n",
            "\n",
            "Rank 2: Similarity = 0.7404\n",
            "Text: I don't have much to say about this place. It was average. But biriyani was good\n",
            "\n",
            "Rank 3: Similarity = 0.6162\n",
            "Text: The ambiance here is cozy and inviting. I enjoyed my meal.\n",
            "\n",
            "Rank 4: Similarity = 0.6040\n",
            "Text: The restaurent ambience is pleasant but need to work on the flavours.\n",
            "\n",
            "Rank 5: Similarity = 0.5784\n",
            "Text: Service was terrible. Waited for an hour and the waiter was rude.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query = \"I want to eat biriyani for lunch.\"\n",
        "\n",
        "# here we are loading pre-trained BERT model and tokenizer\n",
        "name_of_model = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_of_model)\n",
        "model = AutoModel.from_pretrained(name_of_model)\n",
        "\n",
        "# Here we are encoding the query and sample data into BERT embeddings\n",
        "def get_bert_embeddings(text):\n",
        "    inp_tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        op_tokens = model(**inp_tokens)\n",
        "    emb = op_tokens.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return emb\n",
        "\n",
        "inpquery_embed = get_bert_embeddings(query)\n",
        "samp_review_embed = [get_bert_embeddings(text) for text in sample_review_data]\n",
        "\n",
        "# here we are calculating the similarity\n",
        "sim_bet_qandsamp = cosine_similarity([inpquery_embed], samp_embed)[0]\n",
        "\n",
        "# here we are ranking the similarity in descending order\n",
        "desc_rank = sorted(enumerate(sim_bet_qandsamp ), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "print(\"Ranked Documents based on Similarity to Query in descending order:\")\n",
        "for rank, (index, similarity) in enumerate(desc_rank):\n",
        "    print(f\"Rank {rank + 1}: Similarity = {similarity:.4f}\")\n",
        "    print(f\"Text: {sample_review_data[index]}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}